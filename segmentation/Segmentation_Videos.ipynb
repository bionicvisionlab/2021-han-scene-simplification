{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Make Segmentation Videos.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rc8iFhterlYr"
      },
      "source": [
        "# Set up Segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07TGccVeFck3"
      },
      "source": [
        "\n",
        "## Environment Setup\n",
        "\n",
        "First, download the code and pretrained models if we are on colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VsLTHwEzdfJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cfe42bd-386a-42d1-fc34-5544a6846326"
      },
      "source": [
        "import glob, os, cv2, shutil, sys\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image, ImageFilter \n",
        "from skimage import morphology"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'=2.0.1'\t        download_ADE20K.sh     install.log   requirements.txt\n",
            " config\t\t        encoder_epoch_20.pth   LICENSE\t     setup.py\n",
            " data\t\t        eval_multipro.py       mit_semseg    teaser\n",
            " decoder_epoch_20.pth   eval.py\t\t       notebooks     test.py\n",
            " demo_test.sh\t        image.png\t       README.md     train.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQv6JnDSFck4"
      },
      "source": [
        "%%bash\n",
        "# Colab-specific setup\n",
        "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit \n",
        "pip install yacs 2>&1 >> install.log\n",
        "git init 2>&1 >> install.log\n",
        "git remote add origin https://github.com/CSAILVision/semantic-segmentation-pytorch.git 2>> install.log\n",
        "git pull origin master 2>&1 >> install.log\n",
        "DOWNLOAD_ONLY=1 ./demo_test.sh 2>> install.log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9RYbVAhFck8"
      },
      "source": [
        "# System libs\n",
        "import os, csv, torch, numpy, scipy.io, PIL.Image, torchvision.transforms\n",
        "# Our libs\n",
        "from mit_semseg.models import ModelBuilder, SegmentationModule\n",
        "from mit_semseg.utils import colorEncode\n",
        "\n",
        "colors = scipy.io.loadmat('data/color150.mat')['colors']\n",
        "names = {}\n",
        "with open('data/object150_info.csv') as f:\n",
        "    reader = csv.reader(f)\n",
        "    next(reader)\n",
        "    for row in reader:\n",
        "        names[int(row[0])] = row[5].split(\";\")[0]\n",
        "\n",
        "def visualize_result(img, pred, index=None):\n",
        "    # filter prediction class if requested\n",
        "    if index is not None:\n",
        "        pred = pred.copy()\n",
        "        pred[pred != index] = -1\n",
        "        print(f'{names[index+1]}:')\n",
        "        \n",
        "    # colorize prediction\n",
        "    pred_color = colorEncode(pred, colors).astype(numpy.uint8)\n",
        "\n",
        "    # aggregate images and save\n",
        "    im_vis = numpy.concatenate((img, pred_color), axis=1)\n",
        "    display(PIL.Image.fromarray(im_vis))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETpVQSoGFck_"
      },
      "source": [
        "## Loading the segmentation model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-UIhsfwFck_"
      },
      "source": [
        "# Network Builders\n",
        "net_encoder = ModelBuilder.build_encoder(\n",
        "    arch='resnet50dilated',\n",
        "    fc_dim=2048,\n",
        "    weights='encoder_epoch_20.pth')\n",
        "net_decoder = ModelBuilder.build_decoder(\n",
        "    arch='ppm_deepsup',\n",
        "    fc_dim=2048,\n",
        "    num_class=150,\n",
        "    weights='decoder_epoch_20.pth',\n",
        "    use_softmax=True)\n",
        "\n",
        "crit = torch.nn.NLLLoss(ignore_index=-1)\n",
        "segmentation_module = SegmentationModule(net_encoder, net_decoder, crit)\n",
        "segmentation_module.eval()\n",
        "segmentation_module.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWqfXmDkFclB"
      },
      "source": [
        "## Load test data\n",
        "\n",
        "Now we load and normalize a single test image.  Here we use the commonplace convention of normalizing the image to a scale for which the RGB values of a large photo dataset would have zero mean and unit standard deviation.  (These numbers come from the imagenet dataset.)  With this normalization, the limiiting ranges of RGB values are within about (-2.2 to +2.7)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lh-4yPnFclC"
      },
      "source": [
        "# Load and normalize one image as a singleton tensor batch\n",
        "pil_to_tensor = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406], # These are RGB mean+std values\n",
        "        std=[0.229, 0.224, 0.225])  # across a large photo dataset.\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50gJzAwQqp24"
      },
      "source": [
        "def get_houghlines(edges):\n",
        "  kernel = np.ones((10,10), np.uint8)\n",
        "  edge_history = cv2.HoughLinesP(edges.astype(\"uint8\"),1,np.pi/180,15,minLineLength=minLineLength,maxLineGap=maxLineGap)\n",
        "  edge_combined = np.zeros(edges.shape)\n",
        "  try:\n",
        "    for x in range(0, len(edge_history)):\n",
        "      for x1,y1,x2,y2 in edge_history[x]:\n",
        "        if np.abs(x1-x2)>5 and np.abs(y1-y2)>5:# we don't want edges in the border\n",
        "          cv2.line(edge_combined,(x1,y1),(x2,y2),color = (255, 255, 255))\n",
        "    edge_combined = cv2.dilate(edge_combined, kernel, iterations=1)\n",
        "  except (RuntimeError, TypeError, NameError):\n",
        "    print(\"no lines\")\n",
        "  return edge_combined"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM54r6jlKTII"
      },
      "source": [
        "# Install detectron2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_FzH13EjseR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "271ad39c-ec81-4ceb-d40f-8435a2310353"
      },
      "source": [
        "!git clone https://github.com/facebookresearch/detectron2\n",
        "# install dependencies: \n",
        "!pip install pyyaml==5.1 pycocotools>=2.0.1\n",
        "import torch, torchvision\n",
        "print(torch.__version__, torch.cuda.is_available())\n",
        "!gcc --version\n",
        "# opencv is pre-installed on colab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.7.0+cu101 True\n",
            "gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\n",
            "Copyright (C) 2017 Free Software Foundation, Inc.\n",
            "This is free software; see the source for copying conditions.  There is NO\n",
            "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt487J4k07Kk"
      },
      "source": [
        "import torch\n",
        "assert torch.__version__.startswith(\"1.7\")\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.7/index.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmMAr3v40_W9",
        "outputId": "6f6c061d-c777-45b5-eb2d-3cac13a260ae"
      },
      "source": [
        "# Some basic setup:\n",
        "# Setup detectron2 logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "# import some common libraries\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "** fvcore version of PathManager will be deprecated soon. **\n",
            "** Please migrate to the version in iopath repo. **\n",
            "https://github.com/facebookresearch/iopath \n",
            "\n",
            "** fvcore version of PathManager will be deprecated soon. **\n",
            "** Please migrate to the version in iopath repo. **\n",
            "https://github.com/facebookresearch/iopath \n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk4gID50K03a"
      },
      "source": [
        "# Pre-trained detectron2 model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI-dkWmiFEAv"
      },
      "source": [
        "cfg = get_cfg()\n",
        "# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
        "# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "# Check classes information\n",
        "MetadataCatalog.get(cfg.DATASETS.TRAIN[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8orbtF4sDrJ"
      },
      "source": [
        "import glob, numpy\n",
        "all_frames = glob.glob(\"VIDEO FRAMES PATH/*.jpg\") # frame1.jpg, frame2.jpg, etc\n",
        "W = 10\n",
        "w_count = 0\n",
        "edge_rep = np.zeros((540, 960, W))\n",
        "\n",
        "for count in np.arange(1,len(all_frames)): #each frame\n",
        "  f_name = \"/frame%d.jpg\" %count\n",
        "\n",
        "  \"\"\" segmentation \"\"\"\n",
        "  pil_image = PIL.Image.open(f_name).convert('RGB')\n",
        "  img_original = numpy.array(pil_image)\n",
        "  img_data = pil_to_tensor(pil_image)\n",
        "  singleton_batch = {'img_data': img_data[None].cuda()}\n",
        "  output_size = img_data.shape[1:]\n",
        "\n",
        "  with torch.no_grad():\n",
        "    scores = segmentation_module(singleton_batch, segSize=output_size)\n",
        "\n",
        "  # Get the predicted scores for each pixel\n",
        "  _, pred = torch.max(scores, dim=1)\n",
        "  pred = pred.cpu()[0].numpy()\n",
        "  # visualize_result(img_original, pred)\n",
        "\n",
        "  # filter out other classes\n",
        "  classes = [6, 11] #road, sidewalk\n",
        "  pred_clean = pred.copy()\n",
        "  pred_clean[~np.isin(pred_clean, classes)]= 0\n",
        "\n",
        "  # filter out small islands\n",
        "  pred_clean2 = morphology.remove_small_objects(pred_clean.astype(bool), min_size=16000).astype(int)*255\n",
        "\n",
        "  # combine mask with correct class labels\n",
        "  pred_clean3 = np.minimum(pred_clean, pred_clean2)\n",
        "\n",
        "  # get structure edges and get only long ones\n",
        "  image = Image.fromarray(np.uint8(pred_clean3 * 255) , 'L')\n",
        "  image_edge = image.filter(ImageFilter.FIND_EDGES) \n",
        "  image_edge = np.array(image_edge)\n",
        "  kernel = np.ones((10,10), np.uint8)\n",
        "  image_edge = cv2.dilate(image_edge, kernel, iterations=1)\n",
        "\n",
        "  minLineLength = 200\n",
        "  maxLineGap = 1\n",
        "  lines = cv2.HoughLinesP(image_edge,1,np.pi/180,15,minLineLength=minLineLength,maxLineGap=maxLineGap)\n",
        "  edges = np.zeros(pred_clean3.shape)\n",
        "  try:\n",
        "    for x in range(0, len(lines)):\n",
        "      for x1,y1,x2,y2 in lines[x]:\n",
        "        if np.abs(x1-x2)>5 and np.abs(y1-y2)>5:# we don't want edges in the border\n",
        "          cv2.line(edges,(x1,y1),(x2,y2),color = (255, 255, 255))\n",
        "    edges = cv2.dilate(edges, kernel, iterations=1)\n",
        "  except (RuntimeError, TypeError, NameError):\n",
        "    print(\"no lines\")\n",
        "\n",
        "  ##############\n",
        "  if count <= W: \n",
        "    edge_rep[:,:,count-1] = edges\n",
        "  else:\n",
        "    #update current edge\n",
        "    hist_curr = np.concatenate([edge_rep, np.expand_dims(edges,2)], axis = 2)\n",
        "    hist_curr = np.max(hist_curr, axis = 2)\n",
        "    plt.imshow(hist_curr)\n",
        "    hist_curr = cv2.erode(get_houghlines(hist_curr), np.ones((10,10)))\n",
        "    plt.imshow(hist_curr)\n",
        "    kernel2 = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))\n",
        "    (thresh, binRed) = cv2.threshold(hist_curr, 0, 255, cv2.THRESH_BINARY)\n",
        "    hist_curr = cv2.morphologyEx(hist_curr, cv2.MORPH_OPEN, kernel2, iterations=3)\n",
        "    hist_curr = cv2.erode(get_houghlines(hist_curr), np.ones((10,10)))\n",
        "    edges = hist_curr\n",
        "  ##############\n",
        "\n",
        "\n",
        "  \"\"\" detectron2 \"\"\"\n",
        "  im = cv2.imread(f_name)\n",
        "  predictor = DefaultPredictor(cfg)\n",
        "  outputs = predictor(im)\n",
        "  instances = outputs[\"instances\"]\n",
        "  important_classes = [0,1,2,5,7] # person, bicycle, car, bus, train\n",
        "  classes = instances.pred_classes if instances.has(\"pred_classes\") else None\n",
        "  if instances.has(\"pred_masks\"):\n",
        "      masks = np.asarray(instances.pred_masks.cpu().numpy())\n",
        "  else:\n",
        "      masks = None\n",
        "  classes_fil = []\n",
        "  for c in classes:\n",
        "    if c in important_classes:\n",
        "      classes_fil.append(1)\n",
        "    else:\n",
        "      classes_fil.append(0)\n",
        "  if np.sum(classes_fil) == 0: #if no important objects, just show edges\n",
        "    # masks_comb = np.zeros([540, 960])\n",
        "    masks_comb = edges\n",
        "  else:\n",
        "    masks_idx = np.where(np.array(classes_fil) == 1)[0]\n",
        "    masks_fil = masks[masks_idx,:,:]\n",
        "    masks_comb = np.max(masks_fil, axis = 0)\n",
        "\n",
        "  if os.path.isdir(\"detectron_mask\")== False:\n",
        "    os.mkdir(\"detectron_mask\")\n",
        "\n",
        "  print(\"frame %d\" %count)\n",
        "  plt.imshow(masks_comb, \"gray\")\n",
        "  plt.axis(\"off\")\n",
        "  filename = \"frame_%d_seg.jpg\" %count\n",
        "  plt.savefig(\"%detectron_mask/%s\" %(filename), bbox_inches='tight', pad_inches=0)\n",
        "\n",
        "print(\"finished\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIWJssKaKZQN"
      },
      "source": [
        "## Make Videos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeUjkHr7WdWH"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import moviepy.editor as mp\n",
        "import glob\n",
        "fps = 20\n",
        "size = (334, 188)\n",
        "\n",
        "vid_pathOut = \"YOUR VIDEO OUTPUT NAME.avi\" \n",
        "out = cv2.VideoWriter(vid_pathOut,cv2.VideoWriter_fourcc(*'DIVX'), fps, size)\n",
        "\n",
        "allframes = glob.glob('IMAGES IN SEGMENTED FRAMES PATH/*.jpg')\n",
        "for i in np.arange(1,len(allframes)):\n",
        "  filename = \"YOUR FRAME NAME\" \n",
        "  img = cv2.imread(filename)\n",
        "  out.write(img)\n",
        "\n",
        "out.release()\n",
        "clip = mp.VideoFileClip(vid_pathOut)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}